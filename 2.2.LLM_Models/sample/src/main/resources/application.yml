spring:
  application:
    name: llm-models-demo
  
  # OpenAI 설정 (기본 모델)
  ai:
    openai:
      api-key: ${OPENAI_API_KEY:your-api-key-here}
      chat:
        options:
          model: gpt-4          # 또는 gpt-3.5-turbo
          temperature: 0.7      # 0.0 ~ 2.0 (창의성 조절)
          max-tokens: 1000      # 최대 토큰 수
          top-p: 1.0
          frequency-penalty: 0.0
          presence-penalty: 0.0

# Anthropic (Claude) 사용 시 - OpenAI 주석 처리 후 활성화
# spring:
#   ai:
#     anthropic:
#       api-key: ${ANTHROPIC_API_KEY}
#       chat:
#         options:
#           model: claude-3-sonnet-20240229
#           temperature: 0.7
#           max-tokens: 1000

# Ollama 사용 시 (로컬 LLM) - 여러 모델 동시 사용 시 함께 활성화 가능
# spring:
#   ai:
#     ollama:
#       base-url: http://localhost:11434
#       chat:
#         options:
#           model: llama2        # 또는 mistral, codellama 등
#           temperature: 0.7

server:
  port: 9000

logging:
  level:
    com.example.springai: DEBUG
    org.springframework.ai: DEBUG

